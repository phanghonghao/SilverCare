
import React, { useEffect, useRef, useState } from 'react';
import { GoogleGenAI, Modality } from '@google/genai';
import { getActiveApiKey } from '../services/geminiService';
import { encode, decode, decodeAudioData } from '../utils/audioUtils';

const FRAME_RATE = 1; // æ¯ç§’å‚³é€ 1 å¹€çµ¦ AI è§€çœ‹
const SAMPLE_RATE = 24000;

const LiveCall: React.FC<{ onEnd: () => void }> = ({ onEnd }) => {
  const [status, setStatus] = useState<'connecting' | 'active' | 'error'>('connecting');
  const [isMuted, setIsMuted] = useState(false);
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const sessionRef = useRef<any>(null);
  const nextStartTimeRef = useRef<number>(0);
  const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());

  useEffect(() => {
    let intervalId: number;
    const apiKey = getActiveApiKey();
    if (!apiKey) {
      setStatus('error');
      return;
    }

    const startCall = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: { facingMode: 'user' } });
        if (videoRef.current) videoRef.current.srcObject = stream;

        const ai = new GoogleGenAI({ apiKey });
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: SAMPLE_RATE });
        const inputCtx = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });

        const sessionPromise = ai.live.connect({
          model: 'gemini-2.5-flash-native-audio-preview-12-2025',
          callbacks: {
            onopen: () => {
              setStatus('active');
              // é–‹å§‹ä¸²æµéŸ³é »
              const source = inputCtx.createMediaStreamSource(stream);
              const processor = inputCtx.createScriptProcessor(4096, 1, 1);
              processor.onaudioprocess = (e) => {
                if (isMuted) return;
                const inputData = e.inputBuffer.getChannelData(0);
                const int16 = new Int16Array(inputData.length);
                for (let i = 0; i < inputData.length; i++) int16[i] = inputData[i] * 32768;
                const blob = { data: encode(new Uint8Array(int16.buffer)), mimeType: 'audio/pcm;rate=16000' };
                sessionPromise.then(s => s.sendRealtimeInput({ media: blob }));
              };
              source.connect(processor);
              processor.connect(inputCtx.destination);
            },
            onmessage: async (msg) => {
              const audioBase64 = msg.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
              if (audioBase64 && audioContextRef.current) {
                const ctx = audioContextRef.current;
                nextStartTimeRef.current = Math.max(nextStartTimeRef.current, ctx.currentTime);
                const buffer = await decodeAudioData(decode(audioBase64), ctx, SAMPLE_RATE, 1);
                const source = ctx.createBufferSource();
                source.buffer = buffer;
                source.connect(ctx.destination);
                source.start(nextStartTimeRef.current);
                nextStartTimeRef.current += buffer.duration;
                sourcesRef.current.add(source);
                source.onended = () => sourcesRef.current.delete(source);
              }
              if (msg.serverContent?.interrupted) {
                sourcesRef.current.forEach(s => s.stop());
                sourcesRef.current.clear();
                nextStartTimeRef.current = 0;
              }
            },
            onclose: () => onEnd(),
            onerror: () => setStatus('error')
          },
          config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Kore' } } },
            systemInstruction: "ä½ æ­£åœ¨èˆ‡ä¸€ä½è€äººé€²è¡Œå³æ™‚è¦–è¨Šé€šè©±ã€‚ä½ å¯ä»¥çœ‹åˆ°ä»–æä¾›çš„æ”åƒé ­ç•«é¢ã€‚è«‹è¡¨ç¾å¾—åƒä¸€å€‹è¦ªåˆ‡çš„å­«å¥³ï¼Œé—œæ³¨ä»–çš„æƒ…ç·’ï¼Œè©¢å•ä»–ä»Šå¤©çš„å¿ƒæƒ…ï¼Œå¦‚æœä»–å±•ç¤ºæ±è¥¿çµ¦ä½ ï¼Œè«‹ç†±æƒ…åœ°å›æ‡‰ã€‚"
          }
        });

        sessionRef.current = await sessionPromise;

        // å®šæ™‚æŠ“å–ç•«é¢ç™¼é€çµ¦ AI
        intervalId = window.setInterval(() => {
          if (videoRef.current && canvasRef.current && sessionRef.current) {
            const canvas = canvasRef.current;
            const video = videoRef.current;
            canvas.width = 320; // é™ä½åˆ†è¾¨ç‡ä»¥æ¸›å°‘å¸¶å¯¬
            canvas.height = (video.videoHeight / video.videoWidth) * 320;
            canvas.getContext('2d')?.drawImage(video, 0, 0, canvas.width, canvas.height);
            const base64 = canvas.toDataURL('image/jpeg', 0.5).split(',')[1];
            sessionRef.current.sendRealtimeInput({ media: { data: base64, mimeType: 'image/jpeg' } });
          }
        }, 1000 / FRAME_RATE);

      } catch (err) {
        console.error(err);
        setStatus('error');
      }
    };

    startCall();

    return () => {
      window.clearInterval(intervalId);
      if (sessionRef.current) sessionRef.current.close();
      if (audioContextRef.current) audioContextRef.current.close();
      sourcesRef.current.forEach(s => s.stop());
    };
  }, []);

  return (
    <div className="fixed inset-0 z-[500] bg-slate-900 flex flex-col overflow-hidden animate-fade-in">
      {/* èƒŒæ™¯è¦–è¨Š */}
      <video ref={videoRef} autoPlay playsInline muted className="absolute inset-0 w-full h-full object-cover opacity-60" />
      
      {/* éš±è—çš„ Canvas ç”¨æ–¼æ“·å–ç•«é¢ */}
      <canvas ref={canvasRef} className="hidden" />

      {/* é ‚éƒ¨ç‹€æ…‹ */}
      <div className="relative z-10 p-8 flex flex-col items-center">
        <div className={`px-6 py-2 rounded-full font-bold text-lg mb-2 flex items-center gap-2 ${
          status === 'active' ? 'bg-green-500/20 text-green-400 border border-green-500/30' : 'bg-blue-500/20 text-blue-400 animate-pulse'
        }`}>
          <div className={`w-3 h-3 rounded-full ${status === 'active' ? 'bg-green-500' : 'bg-blue-500 animate-ping'}`}></div>
          {status === 'active' ? 'æ­£åœ¨èˆ‡å°ç²é€šè©±ä¸­' : 'å°ç²æ­£åœ¨æ¥è½...'}
        </div>
        <h2 className="text-white text-3xl font-black shadow-lg">å°ç² AI é™ªä¼´</h2>
      </div>

      {/* åº•éƒ¨æ§åˆ¶æ¬„ */}
      <div className="relative z-10 mt-auto p-12 bg-gradient-to-t from-black/80 to-transparent flex flex-col items-center gap-8">
        {status === 'error' && (
          <div className="bg-red-500/80 p-4 rounded-2xl text-white font-bold mb-4">
            é€šè©±é€£æ¥å¤±æ•—ï¼Œè«‹ç¢ºèªå·²åœ¨é¦–é é…ç½®å¯†é‘°ä¸¦é–‹å•Ÿæ¬Šé™ã€‚
          </div>
        )}

        <div className="flex justify-center items-center gap-10">
          <button 
            onClick={() => setIsMuted(!isMuted)}
            className={`w-20 h-20 rounded-full flex items-center justify-center text-3xl transition-all ${
              isMuted ? 'bg-red-500 text-white' : 'bg-white/20 text-white backdrop-blur-md'
            }`}
          >
            {isMuted ? 'ğŸ”‡' : 'ğŸ¤'}
          </button>

          <button 
            onClick={onEnd}
            className="w-32 h-32 bg-red-600 hover:bg-red-500 rounded-full flex items-center justify-center text-5xl shadow-[0_0_50px_rgba(220,38,38,0.5)] active:scale-90 transition-all"
          >
            ğŸ“
          </button>

          <button className="w-20 h-20 bg-white/20 backdrop-blur-md rounded-full flex items-center justify-center text-3xl text-white">
            ğŸ”„
          </button>
        </div>
        
        <p className="text-white/60 text-xl font-medium">åƒå¹³æ™‚èŠå¤©ä¸€æ¨£å°æˆ‘èªªè©±å°±å¯ä»¥å“¦</p>
      </div>

      <style>{`
        @keyframes fade-in { from { opacity: 0; } to { opacity: 1; } }
      `}</style>
    </div>
  );
};

export default LiveCall;
